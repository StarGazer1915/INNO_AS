{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec91ae6",
   "metadata": {},
   "source": [
    "# AS3.1 - Deep Q-learning Network (Lunar Lander)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec6e8b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea32a0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch import nn, save, load, from_numpy\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from src.lmodel import Lmodel\n",
    "\n",
    "from src.agent import Agent\n",
    "from src.policy import Policy\n",
    "from src.memory import Memory\n",
    "from src.lmodel import Lmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0e5bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b985cef",
   "metadata": {},
   "source": [
    "### Defining numeric parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5651552",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "max_steps = 2000\n",
    "avg_reward_threshold = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "epsilon = 0.5\n",
    "decay = 0.99\n",
    "\n",
    "memory_size = 32000\n",
    "sample_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316c5a8",
   "metadata": {},
   "source": [
    "### Defining Model, Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4b20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_nn = Lmodel().to('cuda')\n",
    "my_nn = Lmodel()\n",
    "optimizer = Adam(my_nn.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf390a",
   "metadata": {},
   "source": [
    "### Defining Model, Objects and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4373fefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm0 = Lmodel()\n",
    "p0 = Policy(lm0, epsilon)\n",
    "me0 = Memory(memory_size)\n",
    "a0 = Agent(me0, p0, decay, sample_size)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "available_actions = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec78120",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80036a59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "q_values: \n",
      "[0.038753692060709, 0.06982547044754028, 0.17940495908260345, -0.17027626931667328]\n",
      "\n",
      "q_prime_values: \n",
      "[0.03916481137275696, 0.0695347934961319, 0.17954756319522858, -0.16947078704833984]\n",
      "\n",
      "action_prime: 2\n",
      "\n",
      "-2.319281390458816 = -2.497033478022092 + 0.99 * 0.17954756319522858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(num_epochs):\n",
    "    state, info = env.reset(seed=42)\n",
    "    q_values = a0.policy.nn(from_numpy(state)).tolist()\n",
    "    for step in range(max_steps):\n",
    "        # ===== Decide action ===== #\n",
    "        action = a0.policy.select_action(available_actions, q_values)\n",
    "        \n",
    "        # ===== Take action ===== #\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # ===== Store Transition ===== #\n",
    "        transition = (action, reward, state, new_state, terminated)\n",
    "        a0.memory.store(transition)\n",
    "        \n",
    "        # ===== Train NN ===== #\n",
    "        a0.train(available_actions)\n",
    "        \n",
    "        \n",
    "        break\n",
    "    break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964794b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## TEST - Running the Environment example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47570238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# observation, info = env.reset(seed=42)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     action = env.action_space.sample()  # this is where you would insert your policy\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     print(f\"\\n1.) observation: {list(observation)}\\n2.) reward: {reward}\\n\"\n",
    "#           f\"3.) available actions: {env.action_space}\\n4.) performed action: {action}\\n\")\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "#     break\n",
    "\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
