{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec91ae6",
   "metadata": {},
   "source": [
    "# AS3.1 - Deep Q-learning Network (Lunar Lander)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec6e8b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea32a0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from random import randint\n",
    "import torch\n",
    "\n",
    "from src.lmodel import Lmodel\n",
    "from src.agent import Agent\n",
    "from src.policy import Policy\n",
    "from src.memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba7f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0e5bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a0656",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5651552",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "max_steps = 2000\n",
    "avg_reward_threshold = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "discount = 0.99\n",
    "\n",
    "memory_size = 32000\n",
    "sample_size = 64\n",
    "\n",
    "available_actions = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf390a",
   "metadata": {},
   "source": [
    "## Visualizing Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eac96fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lmodel(\n",
      "  (l1): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (l2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (l3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test = Lmodel().to(device)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10178e1",
   "metadata": {},
   "source": [
    "## Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb76b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ddce1",
   "metadata": {},
   "source": [
    "## Defining Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94becb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory class for the Agent\n",
    "me0 = Memory(memory_size)\n",
    "\n",
    "# The Policy class for the Agent\n",
    "p0 = Policy(Lmodel().to(device), learning_rate, epsilon, available_actions, epsilon_decay)\n",
    "\n",
    "# The Agent class\n",
    "a0 = Agent(env.step, me0, p0, device, sample_size, num_epochs, max_steps, discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec78120",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80036a59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Epoch rewards: 11.696699693236255 | Training losses: 31189.942172540825 | Epsilon: 0.99\n",
      "Epoch 1 | Epoch rewards: -170.6104836700173 | Training losses: 231306.9749620823 | Epsilon: 0.9801\n",
      "Epoch 2 | Epoch rewards: -471.4013349848496 | Training losses: 192921.48842534237 | Epsilon: 0.9702989999999999\n",
      "Epoch 3 | Epoch rewards: -346.9902891404704 | Training losses: 200828.80273651084 | Epsilon: 0.96059601\n",
      "Epoch 4 | Epoch rewards: -262.45698033601457 | Training losses: 246448.25836025784 | Epsilon: 0.9509900498999999\n",
      "Epoch 5 | Epoch rewards: -145.30539737772276 | Training losses: 194177.72397519829 | Epsilon: 0.9414801494009999\n",
      "Epoch 6 | Epoch rewards: -98.73329421631539 | Training losses: 170292.03541759972 | Epsilon: 0.9320653479069899\n",
      "Epoch 7 | Epoch rewards: -153.54804943346687 | Training losses: 162581.77649735354 | Epsilon: 0.92274469442792\n",
      "Epoch 8 | Epoch rewards: -332.6737502718716 | Training losses: 160875.97879105527 | Epsilon: 0.9135172474836407\n",
      "Epoch 9 | Epoch rewards: -258.15949981846506 | Training losses: 184813.10127598446 | Epsilon: 0.9043820750088043\n",
      "Epoch 10 | Epoch rewards: -54.96352814018509 | Training losses: 120816.5374729766 | Epsilon: 0.8953382542587163\n",
      "Epoch 11 | Epoch rewards: -159.16396478310185 | Training losses: 202997.54096477738 | Epsilon: 0.8863848717161291\n",
      "Epoch 12 | Epoch rewards: -123.52966865552385 | Training losses: 171376.57784379544 | Epsilon: 0.8775210229989678\n",
      "Epoch 13 | Epoch rewards: -292.8883707106197 | Training losses: 188321.59117590555 | Epsilon: 0.8687458127689781\n",
      "Epoch 14 | Epoch rewards: -388.87988173941585 | Training losses: 198036.15784201512 | Epsilon: 0.8600583546412883\n",
      "Epoch 15 | Epoch rewards: -237.4661740379068 | Training losses: 291629.0190461105 | Epsilon: 0.8514577710948754\n",
      "Epoch 16 | Epoch rewards: -260.6372796402345 | Training losses: 243556.52676439445 | Epsilon: 0.8429431933839266\n",
      "Epoch 17 | Epoch rewards: -106.43074694169117 | Training losses: 172301.38044388752 | Epsilon: 0.8345137614500874\n",
      "Epoch 18 | Epoch rewards: -313.61593374460745 | Training losses: 263771.4193955266 | Epsilon: 0.8261686238355865\n",
      "Epoch 19 | Epoch rewards: -95.07485035728166 | Training losses: 149622.61934373522 | Epsilon: 0.8179069375972307\n",
      "Epoch 20 | Epoch rewards: -383.6695089055418 | Training losses: 262585.3739511749 | Epsilon: 0.8097278682212583\n",
      "Epoch 21 | Epoch rewards: -238.24573988027694 | Training losses: 248523.29920052955 | Epsilon: 0.8016305895390458\n",
      "Epoch 22 | Epoch rewards: -306.78078975866254 | Training losses: 365681.15016850375 | Epsilon: 0.7936142836436553\n",
      "Epoch 23 | Epoch rewards: -382.97663333669004 | Training losses: 171882.85948092624 | Epsilon: 0.7856781408072188\n",
      "Epoch 24 | Epoch rewards: -81.25482000165533 | Training losses: 178499.0517570702 | Epsilon: 0.7778213593991465\n",
      "Epoch 25 | Epoch rewards: -315.2521814779268 | Training losses: 134968.1265311822 | Epsilon: 0.7700431458051551\n",
      "Epoch 26 | Epoch rewards: -155.00025467345756 | Training losses: 205040.20753998208 | Epsilon: 0.7623427143471035\n",
      "Epoch 27 | Epoch rewards: -122.77476403661237 | Training losses: 280175.1856293163 | Epsilon: 0.7547192872036325\n",
      "Epoch 28 | Epoch rewards: -383.8298936269448 | Training losses: 134695.34675485015 | Epsilon: 0.7471720943315961\n",
      "Epoch 29 | Epoch rewards: -99.25868332793388 | Training losses: 277407.92834474973 | Epsilon: 0.7397003733882802\n",
      "Epoch 30 | Epoch rewards: -471.5668434061149 | Training losses: 180758.4280260422 | Epsilon: 0.7323033696543974\n",
      "Epoch 31 | Epoch rewards: -134.71802887664757 | Training losses: 146752.92441878765 | Epsilon: 0.7249803359578534\n",
      "Epoch 32 | Epoch rewards: -106.35568383618967 | Training losses: 167149.51733822015 | Epsilon: 0.7177305325982748\n",
      "Epoch 33 | Epoch rewards: -382.71512142255347 | Training losses: 185304.09733988668 | Epsilon: 0.7105532272722921\n",
      "Epoch 34 | Epoch rewards: -97.1523337432801 | Training losses: 171028.21229699434 | Epsilon: 0.7034476949995692\n",
      "Epoch 35 | Epoch rewards: -423.0396572706517 | Training losses: 188377.18703842754 | Epsilon: 0.6964132180495735\n",
      "Epoch 36 | Epoch rewards: -73.49096599786648 | Training losses: 151189.04044203518 | Epsilon: 0.6894490858690777\n",
      "Epoch 37 | Epoch rewards: -403.0855157331813 | Training losses: 329705.796921381 | Epsilon: 0.682554595010387\n",
      "Epoch 38 | Epoch rewards: -147.46384585776502 | Training losses: 185501.21180590926 | Epsilon: 0.6757290490602831\n",
      "Epoch 39 | Epoch rewards: -197.4694960293066 | Training losses: 129553.23409531999 | Epsilon: 0.6689717585696803\n",
      "Epoch 40 | Epoch rewards: -290.6318818520906 | Training losses: 148629.7069077459 | Epsilon: 0.6622820409839835\n",
      "Epoch 41 | Epoch rewards: -233.87318664437151 | Training losses: 135043.98426518304 | Epsilon: 0.6556592205741436\n",
      "Epoch 42 | Epoch rewards: -139.4000848966313 | Training losses: 206005.57258112685 | Epsilon: 0.6491026283684022\n",
      "Epoch 43 | Epoch rewards: -311.74687228575647 | Training losses: 157351.4766999379 | Epsilon: 0.6426116020847181\n",
      "Epoch 44 | Epoch rewards: -391.70171778735215 | Training losses: 153616.37144842112 | Epsilon: 0.6361854860638709\n",
      "Epoch 45 | Epoch rewards: -365.7663608667258 | Training losses: 171610.72720932835 | Epsilon: 0.6298236312032323\n",
      "Epoch 46 | Epoch rewards: -341.6556286272005 | Training losses: 188911.9549036976 | Epsilon: 0.6235253948912\n",
      "Epoch 47 | Epoch rewards: -272.3331275472718 | Training losses: 286737.0896570892 | Epsilon: 0.617290140942288\n",
      "Epoch 48 | Epoch rewards: -162.83562100158906 | Training losses: 112191.89345986526 | Epsilon: 0.6111172395328651\n",
      "Epoch 49 | Epoch rewards: -263.9000699363653 | Training losses: 242867.20247626945 | Epsilon: 0.6050060671375365\n",
      "Epoch 50 | Epoch rewards: -334.0706118356206 | Training losses: 219606.8327436926 | Epsilon: 0.5989560064661611\n",
      "Epoch 51 | Epoch rewards: -488.36817287911106 | Training losses: 158253.569569779 | Epsilon: 0.5929664464014994\n",
      "Epoch 52 | Epoch rewards: -95.08954288208214 | Training losses: 183175.99045035822 | Epsilon: 0.5870367819374844\n",
      "Epoch 53 | Epoch rewards: -269.8470016273253 | Training losses: 175264.57058126328 | Epsilon: 0.5811664141181095\n",
      "Epoch 54 | Epoch rewards: -520.00475548691 | Training losses: 447504.70037160686 | Epsilon: 0.5753547499769285\n",
      "Epoch 55 | Epoch rewards: -336.03233395361906 | Training losses: 191231.78061222922 | Epsilon: 0.5696012024771592\n",
      "Epoch 56 | Epoch rewards: -207.51900713656022 | Training losses: 162363.47901471623 | Epsilon: 0.5639051904523876\n",
      "Epoch 57 | Epoch rewards: -733.2246192202755 | Training losses: 326686.52990996675 | Epsilon: 0.5582661385478638\n",
      "Epoch 58 | Epoch rewards: -336.78066405126486 | Training losses: 147356.8377475575 | Epsilon: 0.5526834771623851\n",
      "Epoch 59 | Epoch rewards: -285.21616517574137 | Training losses: 203563.9970720226 | Epsilon: 0.5471566423907612\n",
      "Epoch 60 | Epoch rewards: -282.12533904161637 | Training losses: 152061.6349728861 | Epsilon: 0.5416850759668536\n",
      "Epoch 61 | Epoch rewards: -337.9563417341814 | Training losses: 133794.67594213106 | Epsilon: 0.536268225207185\n",
      "Epoch 62 | Epoch rewards: -488.9184581183318 | Training losses: 191330.4923197744 | Epsilon: 0.5309055429551132\n",
      "Epoch 63 | Epoch rewards: -112.49747430067154 | Training losses: 606523.3067894224 | Epsilon: 0.525596487525562\n",
      "Epoch 64 | Epoch rewards: -346.92960246713665 | Training losses: 146597.38928485 | Epsilon: 0.5203405226503064\n",
      "Epoch 65 | Epoch rewards: -453.38065435626777 | Training losses: 154006.3861669794 | Epsilon: 0.5151371174238033\n",
      "Epoch 66 | Epoch rewards: -458.75599155863307 | Training losses: 183150.1066351283 | Epsilon: 0.5099857462495653\n",
      "Epoch 67 | Epoch rewards: -245.16617312190866 | Training losses: 170654.33412607017 | Epsilon: 0.5048858887870696\n",
      "Epoch 68 | Epoch rewards: -431.18598497319175 | Training losses: 207317.90757602794 | Epsilon: 0.4998370298991989\n",
      "Epoch 69 | Epoch rewards: -281.5580759834621 | Training losses: 257552.7526010028 | Epsilon: 0.49483865960020695\n",
      "Epoch 70 | Epoch rewards: -565.169094889314 | Training losses: 243579.98683977025 | Epsilon: 0.4898902730042049\n",
      "Epoch 71 | Epoch rewards: -232.01171507852962 | Training losses: 118238.05814373124 | Epsilon: 0.48499137027416284\n",
      "Epoch 72 | Epoch rewards: -798.6197027668006 | Training losses: 401896.39075980254 | Epsilon: 0.4801414565714212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 | Epoch rewards: -556.1094718095037 | Training losses: 139750.6404932277 | Epsilon: 0.475340042005707\n",
      "Epoch 74 | Epoch rewards: -451.6607062267181 | Training losses: 141333.89342812978 | Epsilon: 0.47058664158564995\n",
      "Epoch 75 | Epoch rewards: -474.18769858108317 | Training losses: 141581.04969435136 | Epsilon: 0.4658807751697934\n",
      "Epoch 76 | Epoch rewards: -336.94088015454804 | Training losses: 164791.4580710476 | Epsilon: 0.4612219674180955\n",
      "Epoch 77 | Epoch rewards: -871.7524813444039 | Training losses: 294015.52077287715 | Epsilon: 0.45660974774391455\n",
      "Epoch 78 | Epoch rewards: -319.467228087012 | Training losses: 266536.346928328 | Epsilon: 0.4520436502664754\n",
      "Epoch 79 | Epoch rewards: -514.9211980693451 | Training losses: 247694.38454810335 | Epsilon: 0.44752321376381066\n",
      "Epoch 80 | Epoch rewards: -455.88736368936804 | Training losses: 116451.037508512 | Epsilon: 0.44304798162617254\n",
      "Epoch 81 | Epoch rewards: -474.6784079343268 | Training losses: 155572.04932295717 | Epsilon: 0.4386175018099108\n",
      "Epoch 82 | Epoch rewards: -446.03239464552223 | Training losses: 159828.4930922144 | Epsilon: 0.4342313267918117\n",
      "Epoch 83 | Epoch rewards: -457.19130629810354 | Training losses: 150099.90190384167 | Epsilon: 0.4298890135238936\n",
      "Epoch 84 | Epoch rewards: -469.02198365685877 | Training losses: 161888.34703234286 | Epsilon: 0.42559012338865465\n",
      "Epoch 85 | Epoch rewards: -314.5640916600412 | Training losses: 122405.65184157317 | Epsilon: 0.4213342221547681\n",
      "Epoch 86 | Epoch rewards: -505.9209672245048 | Training losses: 240657.84842997344 | Epsilon: 0.41712087993322045\n",
      "Epoch 87 | Epoch rewards: -608.16116497138 | Training losses: 226730.84008598194 | Epsilon: 0.41294967113388825\n",
      "Epoch 88 | Epoch rewards: -701.2152480861058 | Training losses: 276976.3934519349 | Epsilon: 0.40882017442254937\n",
      "Epoch 89 | Epoch rewards: -238.05918198074772 | Training losses: 169683.69657064744 | Epsilon: 0.4047319726783239\n",
      "Epoch 90 | Epoch rewards: -382.81047513961624 | Training losses: 197547.63555736773 | Epsilon: 0.40068465295154065\n",
      "Epoch 91 | Epoch rewards: -496.5093019150388 | Training losses: 176699.12107057497 | Epsilon: 0.39667780642202527\n",
      "Epoch 92 | Epoch rewards: -288.71419399291426 | Training losses: 181342.0693345263 | Epsilon: 0.392711028357805\n",
      "Epoch 93 | Epoch rewards: -1133.030968833392 | Training losses: 423164.5112474726 | Epsilon: 0.38878391807422696\n",
      "Epoch 94 | Epoch rewards: -402.87050900053583 | Training losses: 245272.03527087258 | Epsilon: 0.3848960788934847\n",
      "Epoch 95 | Epoch rewards: -464.2071991943831 | Training losses: 245046.62312377137 | Epsilon: 0.38104711810454983\n",
      "Epoch 96 | Epoch rewards: -399.36318740662904 | Training losses: 151597.64337610232 | Epsilon: 0.37723664692350434\n",
      "Epoch 97 | Epoch rewards: -614.1316626440698 | Training losses: 257002.2970125756 | Epsilon: 0.37346428045426927\n",
      "Epoch 98 | Epoch rewards: -655.8956448260882 | Training losses: 341384.8627463491 | Epsilon: 0.36972963764972655\n",
      "Epoch 99 | Epoch rewards: -248.16165280528426 | Training losses: 146481.51858913462 | Epsilon: 0.36603234127322926\n",
      "Epoch 100 | Epoch rewards: -433.2263577020366 | Training losses: 132981.5355688798 | Epsilon: 0.36237201786049694\n",
      "Epoch 100 | Average reward: -339.56335149969556 and Loss: 203285.53195942024 | Epsilon: 0.36237201786049694\n",
      "\n",
      "Epoch 101 | Epoch rewards: -582.1844583636926 | Training losses: 243200.6941912754 | Epsilon: 0.358748297681892\n",
      "Epoch 102 | Epoch rewards: -633.0606532866016 | Training losses: 247746.23986790184 | Epsilon: 0.35516081470507305\n",
      "Epoch 103 | Epoch rewards: -511.7118663595822 | Training losses: 144042.84039245022 | Epsilon: 0.3516092065580223\n",
      "Epoch 104 | Epoch rewards: -305.9420922694411 | Training losses: 195043.71855986872 | Epsilon: 0.34809311449244207\n",
      "Epoch 105 | Epoch rewards: -414.8241168388218 | Training losses: 177600.0954519422 | Epsilon: 0.34461218334751764\n",
      "Epoch 106 | Epoch rewards: -443.16204025245565 | Training losses: 209229.9612549906 | Epsilon: 0.34116606151404244\n",
      "Epoch 107 | Epoch rewards: -435.03404472525983 | Training losses: 280830.97680043353 | Epsilon: 0.337754400898902\n",
      "Epoch 108 | Epoch rewards: -1031.3087346166444 | Training losses: 388828.37627538276 | Epsilon: 0.334376856889913\n",
      "Epoch 109 | Epoch rewards: -456.6468711779982 | Training losses: 134744.97458150817 | Epsilon: 0.33103308832101386\n",
      "Epoch 110 | Epoch rewards: -471.2378959805826 | Training losses: 181697.16862422347 | Epsilon: 0.3277227574378037\n",
      "Epoch 111 | Epoch rewards: -623.350574900404 | Training losses: 183733.66542910595 | Epsilon: 0.3244455298634257\n",
      "Epoch 112 | Epoch rewards: -605.3721466979722 | Training losses: 207249.84655512625 | Epsilon: 0.3212010745647914\n",
      "Epoch 113 | Epoch rewards: -593.6434778362714 | Training losses: 232910.17960942 | Epsilon: 0.3179890638191435\n",
      "Epoch 114 | Epoch rewards: -861.1087411425025 | Training losses: 232846.46693237842 | Epsilon: 0.31480917318095203\n",
      "Epoch 115 | Epoch rewards: -561.2513275116114 | Training losses: 196402.55804868723 | Epsilon: 0.3116610814491425\n",
      "Epoch 116 | Epoch rewards: -1111.1726337647547 | Training losses: 351167.41606358194 | Epsilon: 0.30854447063465107\n",
      "Epoch 117 | Epoch rewards: -880.2713153137764 | Training losses: 264472.31904869515 | Epsilon: 0.30545902592830454\n",
      "Epoch 118 | Epoch rewards: -766.4721101277848 | Training losses: 202900.83941971642 | Epsilon: 0.3024044356690215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "losses = []\n",
    "\n",
    "state, info = env.reset(seed=randint(0, 1000))\n",
    "for i in range(num_epochs):\n",
    "    epoch_reward = 0\n",
    "    epoch_loss = 0\n",
    "    for step in range(max_steps):\n",
    "        state, reward, loss, terminated, truncated = a0.train(state)\n",
    "        epoch_reward += reward\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            state, info = env.reset(seed=randint(0, 1000))\n",
    "            break\n",
    "        \n",
    "    a0.policy.decay()\n",
    "    rewards.append(epoch_reward)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    # ===== Visualization ===== #\n",
    "    print(f\"Epoch {i} | Epoch rewards: {epoch_reward} | Training losses: {epoch_loss} | Epsilon: {a0.policy.epsilon}\")\n",
    "    \n",
    "    if i in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "        run_avg_reward = np.mean(rewards)\n",
    "        run_avg_loss = np.mean(losses)\n",
    "        if run_avg_reward >= 200:\n",
    "            print(f\"Epoch {i} | Average reward: {run_avg_reward} and Loss: {run_avg_loss} | Epsilon: {a0.policy.epsilon}\\n\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Epoch {i} | Average reward: {run_avg_reward} and Loss: {run_avg_loss} | Epsilon: {a0.policy.epsilon}\\n\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
