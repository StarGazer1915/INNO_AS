{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec91ae6",
   "metadata": {},
   "source": [
    "# AS3.1 - Deep Q-learning Network (Lunar Lander)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec6e8b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea32a0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "from torch import nn, save, load, from_numpy\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from src.lmodel import Lmodel\n",
    "\n",
    "from src.agent import Agent\n",
    "from src.policy import Policy\n",
    "from src.memory import Memory\n",
    "from src.lmodel import Lmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3550e568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: False\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0e5bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b985cef",
   "metadata": {},
   "source": [
    "### Defining numeric parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5651552",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "max_steps = 2000\n",
    "avg_reward_threshold = 200\n",
    "\n",
    "learning_rate = 0.01\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "discount = 0.99\n",
    "\n",
    "memory_size = 32000\n",
    "sample_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316c5a8",
   "metadata": {},
   "source": [
    "### Defining Model, Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4b20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_nn = Lmodel().to('cuda')\n",
    "my_nn = Lmodel().to(device)\n",
    "optimizer = Adam(my_nn.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf390a",
   "metadata": {},
   "source": [
    "### Defining Model, Objects and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4373fefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p0 = Policy(my_nn, optimizer, loss_fn, epsilon)\n",
    "me0 = Memory(memory_size)\n",
    "a0 = Agent(me0, p0, discount, epsilon_decay, sample_size)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=None)\n",
    "available_actions = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec78120",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80036a59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Sum step rewards: -191.5768444947546 | Epsilon: 0.99\n",
      "Epoch 1 | Sum step rewards: -96.78412084792654 | Epsilon: 0.9801\n",
      "Epoch 2 | Sum step rewards: -216.53168965676235 | Epsilon: 0.9702989999999999\n",
      "Epoch 3 | Sum step rewards: -100.00775302905596 | Epsilon: 0.96059601\n",
      "Epoch 4 | Sum step rewards: -109.27021175508277 | Epsilon: 0.9509900498999999\n",
      "Epoch 5 | Sum step rewards: -157.42670322647453 | Epsilon: 0.9414801494009999\n",
      "Epoch 6 | Sum step rewards: -118.71451468327619 | Epsilon: 0.9320653479069899\n",
      "Epoch 7 | Sum step rewards: -203.66443735735078 | Epsilon: 0.92274469442792\n",
      "Epoch 8 | Sum step rewards: -122.2490430190277 | Epsilon: 0.9135172474836407\n",
      "Epoch 9 | Sum step rewards: -255.74112288819595 | Epsilon: 0.9043820750088043\n",
      "Epoch 10 | Sum step rewards: -293.53602472348575 | Epsilon: 0.8953382542587163\n",
      "Epoch 11 | Sum step rewards: -182.05349702213846 | Epsilon: 0.8863848717161291\n",
      "Epoch 12 | Sum step rewards: -388.4281895306684 | Epsilon: 0.8775210229989678\n",
      "Epoch 13 | Sum step rewards: -103.44574853977693 | Epsilon: 0.8687458127689781\n",
      "Epoch 14 | Sum step rewards: -193.67195170094467 | Epsilon: 0.8600583546412883\n",
      "Epoch 15 | Sum step rewards: -89.41917769291051 | Epsilon: 0.8514577710948754\n",
      "Epoch 16 | Sum step rewards: -109.52887974819458 | Epsilon: 0.8429431933839266\n",
      "Epoch 17 | Sum step rewards: -110.84947365908386 | Epsilon: 0.8345137614500874\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for i in range(num_epochs):\n",
    "    step_rewards = []\n",
    "    state, info = env.reset(seed=randint(0, 1000))\n",
    "    for step in range(max_steps):\n",
    "        q_values = a0.policy.nn(from_numpy(state)).tolist()\n",
    "        \n",
    "        # ===== Decide action ===== #\n",
    "        action = a0.policy.select_action(available_actions, q_values)\n",
    "        \n",
    "        # ===== Take action, observe result ===== #\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        step_rewards.append(reward)\n",
    "        \n",
    "        # ===== Store Transition ===== #\n",
    "        transition = (action, reward, state, new_state, terminated)\n",
    "        a0.memory.store(transition)\n",
    "        \n",
    "        # ===== Train NN ===== #\n",
    "        a0.train(available_actions)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "            \n",
    "    rewards.append(sum(step_rewards))\n",
    "    a0.decay_epsilon()\n",
    "    \n",
    "    print(f\"Epoch {i} | Sum step rewards: {sum(step_rewards)} | Epsilon: {a0.policy.epsilon}\")\n",
    "    \n",
    "    if i in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "        run_avg_reward = np.mean(rewards)\n",
    "        if run_avg_reward >= 200:\n",
    "            print(f\"\\nEpoch {i} | Average Reward: {run_avg_reward} | Epsilon: {a0.policy.epsilon}\\n\")\n",
    "            rewards = []\n",
    "            break\n",
    "        else:\n",
    "            print(f\"\\nEpoch {i} | Average Reward: {run_avg_reward} | Epsilon: {a0.policy.epsilon}\\n\")\n",
    "            rewards = []\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964794b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## TEST - Running the Environment example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47570238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# observation, info = env.reset(seed=42)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     action = env.action_space.sample()  # this is where you would insert your policy\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     print(f\"\\n1.) observation: {list(observation)}\\n2.) reward: {reward}\\n\"\n",
    "#           f\"3.) available actions: {env.action_space}\\n4.) performed action: {action}\\n\")\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "#     break\n",
    "\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
