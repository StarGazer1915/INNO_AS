{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec91ae6",
   "metadata": {},
   "source": [
    "# AS3.1 - Deep Q-learning Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec6e8b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea32a0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch import nn, save, load\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from src.lmodel import Lmodel\n",
    "\n",
    "from src.agent import Agent\n",
    "from src.policy import Policy\n",
    "from src.memory import Memory\n",
    "from src.lmodel import Lmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0e5bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b985cef",
   "metadata": {},
   "source": [
    "### Defining numeric parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9cb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epsilon = 1.0\n",
    "decay = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62f69c",
   "metadata": {},
   "source": [
    "### Defining Model, Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb185941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_nn = Lmodel().to('cuda')\n",
    "my_nn = Lmodel()\n",
    "optimizer = Adam(my_nn.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d71cc",
   "metadata": {},
   "source": [
    "### Defining Model and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4373fefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm0 = Lmodel()\n",
    "p0 = Policy(lm0, epsilon)\n",
    "me0 = Memory()\n",
    "a0 = Agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964794b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Creating the Environment (Lunar Lander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47570238",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.) observation: [0.004498291, 1.4247646, 0.22032492, 0.294843, -0.002850535, -0.0039127944, 0.0, 0.0]\n",
      "2.) reward: 2.0804712989958616\n",
      "3.) available actions: Discrete(4)\n",
      "4.) performed action: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"\\n1.) observation: {list(observation)}\\n2.) reward: {reward}\\n\"\n",
    "          f\"3.) available actions: {env.action_space}\\n4.) performed action: {action}\\n\")\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "    break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
