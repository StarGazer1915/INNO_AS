{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec91ae6",
   "metadata": {},
   "source": [
    "# AS3.1 - Deep Q-learning Network (Lunar Lander)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec6e8b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea32a0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch import nn, save, load, from_numpy\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from src.lmodel import Lmodel\n",
    "\n",
    "from src.agent import Agent\n",
    "from src.policy import Policy\n",
    "from src.memory import Memory\n",
    "from src.lmodel import Lmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0e5bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b985cef",
   "metadata": {},
   "source": [
    "### Defining numeric parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5651552",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "max_steps = 2000\n",
    "avg_reward_threshold = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "epsilon = 0.3\n",
    "decay = 0.99\n",
    "\n",
    "memory_size = 32000\n",
    "sample_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4316c5a8",
   "metadata": {},
   "source": [
    "### Defining Model, Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4b20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_nn = Lmodel().to('cuda')\n",
    "my_nn = Lmodel()\n",
    "optimizer = Adam(my_nn.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf390a",
   "metadata": {},
   "source": [
    "### Defining Model, Objects and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4373fefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p0 = Policy(my_nn, optimizer, loss_fn, epsilon)\n",
    "me0 = Memory(memory_size)\n",
    "a0 = Agent(me0, p0, decay, sample_size)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "available_actions = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d1d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec78120",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80036a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 | Average Reward: -69310.40772150125 | Average step loss is: 47.057708648001466 | Epsilon: 0.10871160535814905\n",
      "Epoch 200 | Average Reward: -70612.13368178638 | Average step loss is: 51.13850073830556 | Epsilon: 0.03979196343281463\n",
      "Epoch 300 | Average Reward: -71930.91923615162 | Average step loss is: 52.75057803203375 | Epsilon: 0.014565145539171856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "losses = []\n",
    "for i in range(num_epochs):\n",
    "    epoch_reward = 0\n",
    "    state, info = env.reset(seed=42)\n",
    "    q_values = a0.policy.nn(from_numpy(state)).tolist()\n",
    "    for step in range(max_steps):\n",
    "        # ===== Decide action ===== #\n",
    "        action = a0.policy.select_action(available_actions, q_values)\n",
    "        \n",
    "        # ===== Take action, observe result ===== #\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # ===== Store Transition ===== #\n",
    "        transition = (action, reward, state, new_state, terminated)\n",
    "        a0.memory.store(transition)\n",
    "        \n",
    "        # ===== Train NN ===== #\n",
    "        avg_step_loss = a0.train(available_actions)\n",
    "        losses.append(avg_step_loss)\n",
    "        \n",
    "        state = new_state\n",
    "        q_values = a0.policy.nn(from_numpy(state)).tolist()\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "            \n",
    "    a0.decay_epsilon(decay)\n",
    "    \n",
    "    \n",
    "    if i in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "        run_avg_reward = np.mean(rewards)\n",
    "        run_avg_loss = np.mean(losses)\n",
    "        if run_avg_reward >= 200:\n",
    "            print(f\"Epoch {i} | Average Reward: {run_avg_reward} | Average step loss is: {run_avg_loss} | Epsilon: {a0.policy.epsilon}\") \n",
    "            rewards = []\n",
    "            losses = []\n",
    "            break\n",
    "        else:\n",
    "            rewards = []\n",
    "            losses = []\n",
    "            print(f\"Epoch {i} | Average Reward: {run_avg_reward} | Average step loss is: {run_avg_loss} | Epsilon: {a0.policy.epsilon}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964794b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## TEST - Running the Environment example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47570238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# observation, info = env.reset(seed=42)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     action = env.action_space.sample()  # this is where you would insert your policy\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     print(f\"\\n1.) observation: {list(observation)}\\n2.) reward: {reward}\\n\"\n",
    "#           f\"3.) available actions: {env.action_space}\\n4.) performed action: {action}\\n\")\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "#     break\n",
    "\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
