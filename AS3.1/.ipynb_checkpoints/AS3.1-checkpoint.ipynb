{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec91ae6",
   "metadata": {},
   "source": [
    "# AS3.1 - Deep Q-learning Network (Lunar Lander)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec6e8b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea32a0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from random import randint\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from src.lmodel import Lmodel\n",
    "from src.agent import Agent\n",
    "from src.policy import Policy\n",
    "from src.memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba7f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0e5bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a0656",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5651552",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "max_steps = 2000\n",
    "avg_reward_threshold = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.95\n",
    "discount = 0.99\n",
    "\n",
    "memory_size = 32000\n",
    "sample_size = 64\n",
    "\n",
    "available_actions = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10178e1",
   "metadata": {},
   "source": [
    "## Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb76b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf390a",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eac96fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lmodel(\n",
      "  (l1): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (l2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (l3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Lmodel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ddce1",
   "metadata": {},
   "source": [
    "## Defining Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94becb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory class for the Agent\n",
    "me0 = Memory(memory_size)\n",
    "\n",
    "# The Policy class for the Agent\n",
    "p0 = Policy(model, learning_rate, epsilon, available_actions, epsilon_decay)\n",
    "\n",
    "# The Agent class\n",
    "a0 = Agent(me0, p0, device, sample_size, num_epochs, max_steps, discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7b8056",
   "metadata": {},
   "source": [
    "## Set Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab962bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0.policy.opt = Adam(a0.policy.nn.parameters(), lr=learning_rate)\n",
    "a0.policy.loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec78120",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80036a59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Epoch rewards: -240.07535064982642 | Epsilon: 0.954553460121176\n",
      "Epoch 1 | Epoch rewards: -112.18218462362411 | Epsilon: 0.916198822899158\n",
      "Epoch 2 | Epoch rewards: -135.4981554940534 | Epsilon: 0.8824693157914677\n",
      "Epoch 3 | Epoch rewards: -15.549075873944123 | Epsilon: 0.8373238298757932\n",
      "Epoch 4 | Epoch rewards: -130.22964585204483 | Epsilon: 0.8040816034112525\n",
      "Epoch 5 | Epoch rewards: -409.2936809601199 | Epsilon: 0.7667715297639581\n",
      "Epoch 6 | Epoch rewards: -135.15735560199576 | Epsilon: 0.7268175639642518\n",
      "Epoch 7 | Epoch rewards: 6.734081570621991 | Epsilon: 0.6927460614810248\n",
      "Epoch 8 | Epoch rewards: -246.81136179829525 | Epsilon: 0.6559928364534608\n",
      "Epoch 9 | Epoch rewards: -118.476717618416 | Epsilon: 0.6362822417909691\n",
      "Epoch 10 | Epoch rewards: -463.5239554880575 | Epsilon: 0.6079732367586781\n",
      "Epoch 11 | Epoch rewards: -332.1573727996125 | Epsilon: 0.5584190160629583\n",
      "Epoch 12 | Epoch rewards: -355.77370659345473 | Epsilon: 0.5309123683211118\n",
      "Epoch 13 | Epoch rewards: -340.20244779285775 | Epsilon: 0.5067842382020455\n",
      "Epoch 14 | Epoch rewards: -393.49860129031657 | Epsilon: 0.46571050878033315\n",
      "Epoch 15 | Epoch rewards: -250.1447412280542 | Epsilon: 0.4396811633527356\n",
      "Epoch 16 | Epoch rewards: -357.3923741672605 | Epsilon: 0.4030368827560863\n",
      "Epoch 17 | Epoch rewards: -346.5078861654381 | Epsilon: 0.38800513539028453\n",
      "Epoch 18 | Epoch rewards: -300.4543235465321 | Epsilon: 0.370742294140148\n",
      "Epoch 19 | Epoch rewards: -714.5828817193769 | Epsilon: 0.35495688012387155\n",
      "Epoch 20 | Epoch rewards: -904.9081563446363 | Epsilon: 0.3189293352770884\n",
      "Epoch 21 | Epoch rewards: -149.25618138234353 | Epsilon: 0.3107420219628232\n",
      "Epoch 22 | Epoch rewards: -444.73645450101264 | Epsilon: 0.2996017007995348\n",
      "Epoch 23 | Epoch rewards: -335.97615955046854 | Epsilon: 0.2831395254157089\n",
      "Epoch 24 | Epoch rewards: -305.0566890314846 | Epsilon: 0.27285227888030594\n",
      "Epoch 25 | Epoch rewards: -237.50563195468249 | Epsilon: 0.2613654951140963\n",
      "Epoch 26 | Epoch rewards: -244.77449754310445 | Epsilon: 0.2460179896962517\n",
      "Epoch 27 | Epoch rewards: -547.344156401205 | Epsilon: 0.23378266643365464\n",
      "Epoch 28 | Epoch rewards: -755.3029150489896 | Epsilon: 0.2171033270396432\n",
      "Epoch 29 | Epoch rewards: -314.2988583111285 | Epsilon: 0.21131853706578424\n",
      "Epoch 30 | Epoch rewards: -482.5758836261809 | Epsilon: 0.20211877521720772\n",
      "Epoch 31 | Epoch rewards: -241.40113554363094 | Epsilon: 0.1965365695137673\n",
      "Epoch 32 | Epoch rewards: -677.280851343243 | Epsilon: 0.1833379438648595\n",
      "Epoch 33 | Epoch rewards: -471.00284162330945 | Epsilon: 0.17448150691629763\n",
      "Epoch 34 | Epoch rewards: -371.46437867208556 | Epsilon: 0.16814210843464356\n",
      "Epoch 35 | Epoch rewards: -390.47122096215116 | Epsilon: 0.16317153393265063\n",
      "Epoch 36 | Epoch rewards: -1479.9412415406828 | Epsilon: 0.14435439719357882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a0.policy.nn.train(mode=True)\n",
    "rewards = []\n",
    "for i in range(num_epochs+1):\n",
    "    epoch_reward = 0\n",
    "    state = torch.tensor(env.reset(seed=randint(0, 1000))[0], requires_grad=True).to(device)\n",
    "    for step in range(max_steps):\n",
    "        # ===== Calculate q-values ===== #\n",
    "        state_q = a0.policy.nn(state).detach().numpy().copy()\n",
    "        \n",
    "        # ===== Decide action ===== #\n",
    "        action = a0.policy.select_action(state_q)\n",
    "        \n",
    "        # ===== Take action, observe result ===== #\n",
    "        obs = env.step(action)        \n",
    "        new_state = torch.tensor(obs[0].copy(), requires_grad=True).to(device)\n",
    "        reward, terminated, truncated, info = obs[1], obs[2], obs[3], obs[4]\n",
    "        \n",
    "        # ===== Store Transition ===== #\n",
    "        transition = (state, new_state, action, reward, terminated)\n",
    "        a0.memory.store(transition)\n",
    "        \n",
    "        # ===== Train the model ===== #\n",
    "        a0.train()\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "            \n",
    "        epoch_reward += reward\n",
    "        a0.policy.decay()\n",
    "    \n",
    "    rewards.append(epoch_reward)\n",
    "    \n",
    "    # ===== Visualization ===== #\n",
    "    print(f\"Epoch {i} | Epoch rewards: {epoch_reward} | Epsilon: {a0.policy.epsilon}\")\n",
    "    \n",
    "    if i in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "        run_avg_reward = np.mean(rewards)\n",
    "        if run_avg_reward >= 200:\n",
    "            print(f\"\\nTraining done at Epoch {i} | Average reward: {run_avg_reward} | Epsilon is now: {a0.policy.epsilon}\\n\")\n",
    "            rewards = []\n",
    "            break\n",
    "        else:\n",
    "            print(f\"\\nEpoch {i-100}-{i} | Average reward: {run_avg_reward} | Epsilon is now: {a0.policy.epsilon}\\n\")\n",
    "            rewards = []\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
